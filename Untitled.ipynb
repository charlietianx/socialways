{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train.py\n",
    "# import wandb\n",
    "# from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# import sys\n",
    "# sys.path.insert(0, '/content/drive/Othercomputers/My MacBook Pro/projs/socialways')\n",
    "# %pwd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import chain\n",
    "from torch.autograd import Variable\n",
    "from utils.parse_utils import Scale\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.linear_models import predict_cv\n",
    "\n",
    "# wandb.init(project='socialways')\n",
    "# config = wandb.config\n",
    "# config.batch_size = 256\n",
    "# config.test_batch_size = 256\n",
    "# config.epochs = 10000\n",
    "# config.lr = 1E-4\n",
    "# config.log_interval = 10\n",
    "\n",
    "# Parser arguments\n",
    "parser = argparse.ArgumentParser(description='Social Ways trajectory prediction.')\n",
    "#parser.add_argument('-f', '')\n",
    "parser.add_argument('--batch-size', '--b',\n",
    "                    type=int, default=256, metavar='N',\n",
    "                    help='input batch size for training (default: 256)')\n",
    "parser.add_argument('--epochs', '--e',\n",
    "                    type=int, default=10000, metavar='N',\n",
    "                    help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--model', '--m',\n",
    "                    default='socialWays',\n",
    "                    choices=['socialWays'],\n",
    "                    help='pick a specific network to train'\n",
    "                         '(default: \"socialWays\")')\n",
    "parser.add_argument('--latent-dim', '--ld',\n",
    "                    type=int, default=10, metavar='N',\n",
    "                    help='dimension of latent space (default: 10)')\n",
    "parser.add_argument('--d-learning-rate', '--d-lr',\n",
    "                    type=float, default=1E-3, metavar='N',\n",
    "                    help='learning rate of discriminator (default: 1E-3)')\n",
    "parser.add_argument('--g-learning-rate', '--g-lr',\n",
    "                    type=float, default=1E-4, metavar='N',\n",
    "                    help='learning rate of generator (default: 1E-4)')\n",
    "parser.add_argument('--unrolling-steps', '--unroll',\n",
    "                    type=int, default=1, metavar='N',\n",
    "                    help='number of steps to unroll gan (default: 1)')\n",
    "parser.add_argument('--hidden-size', '--h-size',\n",
    "                    type=int, default=64, metavar='N',\n",
    "                    help='size of network intermediate layer (default: 64)')\n",
    "parser.add_argument('--dataset', '--data',\n",
    "                    default='hotel',\n",
    "                    choices=['hotel'],\n",
    "                    help='pick a specific dataset (default: \"hotel\")')\n",
    "#args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "# ========== set input/output files ============\n",
    "dataset_name = args.dataset\n",
    "model_name = args.model\n",
    "# input_file = '../hotel-8-12.npz'\n",
    "# input_file = 'traj-datasets/seq_eth/eth-8-12.npz'\n",
    "input_file = 'traj-datasets/data_zara01/zara01-8-12.npz'\n",
    "# input_file = '/content/drive/Othercomputers/My MacBook Pro/projs/socialways/traj-datasets/data_zara01/zara01-8-12.npz'\n",
    "#model_file = '../trained_models/' + model_name + '-' + dataset_name + '.pt'\n",
    "# model_file = '/content/drive/Othercomputers/My MacBook Pro/projs/socialways/' + model_name + '-' + dataset_name + '.pt'\n",
    "model_file = 'models/' + model_name + '-' + dataset_name + '.pt'\n",
    "\n",
    "# FIXME: ====== training hyper-parameters ======\n",
    "# Unrolled GAN\n",
    "n_unrolling_steps = args.unrolling_steps\n",
    "# Info GAN\n",
    "use_info_loss = True\n",
    "loss_info_w = 0.5\n",
    "n_latent_codes = 3\n",
    "# L2 GAN\n",
    "use_l2_loss = False\n",
    "use_variety_loss = False\n",
    "loss_l2_w = 0.5  # WARNING for both l2 and variety\n",
    "# Learning Rate\n",
    "lr_g = args.g_learning_rate\n",
    "lr_d = args.d_learning_rate\n",
    "# FIXME: ====== Network Size ===================\n",
    "# Batch size\n",
    "batch_size = args.batch_size\n",
    "# LSTM hidden size\n",
    "hidden_size = args.hidden_size\n",
    "n_epochs = args.epochs\n",
    "num_social_features = 3\n",
    "social_feature_size = args.hidden_size\n",
    "noise_len = args.hidden_size // 2\n",
    "n_lstm_layers = 1\n",
    "use_social = False\n",
    "# ==============================================\n",
    "\n",
    "# FIXME: ======= Loda Data =====================\n",
    "# print(os.path.dirname(os.path.realpath(__file__)))\n",
    "\n",
    "data = np.load(input_file)\n",
    "# Data come as NxTx2 numpy nd-arrays where N is the number of trajectories,\n",
    "# T is their duration.\n",
    "dataset_obsv, dataset_pred, dataset_t, the_batches = \\\n",
    "    data['obsvs'], data['preds'], data['times'], data['batches']\n",
    "# 4/5 of the batches to be used for training\n",
    "train_size = max(1, (len(the_batches) * 4) // 5)\n",
    "train_batches = the_batches[:train_size]\n",
    "# Test batches are the remaining ones\n",
    "test_batches = the_batches[train_size:]\n",
    "# Size of the observed sub-paths\n",
    "n_past = dataset_obsv.shape[1]\n",
    "# Size of the sub-paths to predict\n",
    "n_next = dataset_pred.shape[1]\n",
    "# Number of training samples\n",
    "n_train_samples = the_batches[train_size - 1][1]\n",
    "# Number of testing samples (the remaining ones)\n",
    "n_test_samples = dataset_obsv.shape[0] - n_train_samples\n",
    "if n_test_samples == 0:\n",
    "    n_test_samples = 1\n",
    "    the_batches = np.array([the_batches[0], the_batches[0]])\n",
    "print(input_file, ' # Training samples: ', n_train_samples)\n",
    "\n",
    "# Normalize the spatial data\n",
    "scale = Scale()\n",
    "scale.max_x = max(np.max(dataset_obsv[:, :, 0]), np.max(dataset_pred[:, :, 0]))\n",
    "scale.min_x = min(np.min(dataset_obsv[:, :, 0]), np.min(dataset_pred[:, :, 0]))\n",
    "scale.max_y = max(np.max(dataset_obsv[:, :, 1]), np.max(dataset_pred[:, :, 1]))\n",
    "scale.min_y = min(np.min(dataset_obsv[:, :, 1]), np.min(dataset_pred[:, :, 1]))\n",
    "scale.calc_scale(keep_ratio=True)\n",
    "dataset_obsv = scale.normalize(dataset_obsv)\n",
    "dataset_pred = scale.normalize(dataset_pred)\n",
    "ss = scale.sx\n",
    "# Copy normalized observations/paths to predict into torch GPU tensors\n",
    "dataset_obsv = torch.FloatTensor(dataset_obsv).cuda()\n",
    "dataset_pred = torch.FloatTensor(dataset_pred).cuda()\n",
    "\n",
    "\n",
    "# ================================================\n",
    "\n",
    "# Augment tensors of positions into positions+velocity\n",
    "def get_traj_4d(obsv_p, pred_p):\n",
    "    obsv_v = obsv_p[:, 1:] - obsv_p[:, :-1]\n",
    "    obsv_v = torch.cat([obsv_v[:, 0].unsqueeze(1), obsv_v], dim=1)\n",
    "    obsv_4d = torch.cat([obsv_p, obsv_v], dim=2)\n",
    "    if len(pred_p) == 0: return obsv_4d\n",
    "    pred_p_1 = torch.cat([obsv_p[:, -1].unsqueeze(1), pred_p[:, :-1]], dim=1)\n",
    "    pred_v = pred_p - pred_p_1\n",
    "    pred_4d = torch.cat([pred_p, pred_v], dim=2)\n",
    "    return obsv_4d, pred_4d\n",
    "\n",
    "\n",
    "# Evaluate the error between the model prediction and the true path\n",
    "def calc_error(pred_hat, pred):\n",
    "    N = pred.size(0)\n",
    "    T = pred.size(1)\n",
    "    err_all = torch.pow((pred_hat - pred) / ss, 2).sum(dim=2).sqrt()  # N x T\n",
    "    FDEs = err_all.sum(dim=0).item() / N\n",
    "    ADEs = torch.cumsum(FDEs)\n",
    "    for ii in range(T):\n",
    "        ADEs[ii] /= (ii + 1)\n",
    "    return ADEs.data.cpu().numpy(), FDEs.data().cpu().numpy()\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, h_dim, f_dim):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.f_dim = f_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.W = nn.Linear(h_dim, f_dim, bias=True)\n",
    "\n",
    "    def forward(self, f, h, sub_batches):\n",
    "        Wh = self.W(h)\n",
    "        S = torch.zeros_like(h)\n",
    "        for sb in sub_batches:\n",
    "            N = sb[1] - sb[0]\n",
    "            if N == 1: continue\n",
    "\n",
    "            for ii in range(sb[0], sb[1]):\n",
    "                fi = f[ii, sb[0]:sb[1]]\n",
    "                sigma_i = torch.bmm(fi.unsqueeze(1), Wh[sb[0]:sb[1]]. unsqueeze(2))\n",
    "                sigma_i[ii-sb[0]] = -1000\n",
    "\n",
    "                attentions = torch.softmax(sigma_i.squeeze(), dim=0)\n",
    "                S[ii] = torch.mm(attentions.view(1, N), h[sb[0]:sb[1]])\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "class EmbedSocialFeatures(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EmbedSocialFeatures, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Sequential(nn.Linear(input_size, 32), nn.ReLU(),\n",
    "                                nn.Linear(32, 64), nn.ReLU(),\n",
    "                                nn.Linear(64, hidden_size))\n",
    "\n",
    "    def forward(self, ftr_list, sub_batches):\n",
    "        embedded_features = self.fc(ftr_list)\n",
    "        return embedded_features\n",
    "\n",
    "\n",
    "def DCA(xA_4d, xB_4d):\n",
    "    dp = xA_4d[:2] - xB_4d[:2]\n",
    "    dv = xA_4d[2:] - xB_4d[2:]\n",
    "    ttca = torch.dot(-dp, dv) / (torch.norm(dv) ** 2 + 1E-6)\n",
    "    # ttca = torch.max(ttca, 0)\n",
    "    dca = torch.norm(dp + ttca * dv)\n",
    "    return dca\n",
    "\n",
    "\n",
    "def Bearing(xA_4d, xB_4d):\n",
    "    dp = xA_4d[:2] - xB_4d[:2]\n",
    "    v = xA_4d[2:]\n",
    "    cos_theta = torch.dot(dp, v) / (torch.norm(dp) * torch.norm(v) + 1E-6)\n",
    "    return cos_theta\n",
    "\n",
    "\n",
    "def DCA_MTX(x_4d, D_4d):\n",
    "    Dp = D_4d[:, :, :2]\n",
    "    Dv = D_4d[:, :, 2:]\n",
    "    DOT_Dp_Dv = torch.mul(Dp[:,:,0], Dv[:,:,0]) + torch.mul(Dp[:,:,1], Dv[:,:,1])\n",
    "    Dv_sq = torch.mul(Dv[:,:,0], Dv[:,:,0]) + torch.mul(Dv[:,:,1], Dv[:,:,1]) + 1E-6\n",
    "    TTCA = -torch.div(DOT_Dp_Dv, Dv_sq)\n",
    "    DCA = torch.zeros_like(Dp)\n",
    "    DCA[:, :, 0] = Dp[:, :, 0] + TTCA * Dv[:, :, 0]\n",
    "    DCA[:, :, 1] = Dp[:, :, 1] + TTCA * Dv[:, :, 1]\n",
    "    DCA = torch.norm(DCA, dim=2)\n",
    "    return DCA\n",
    "\n",
    "\n",
    "def BearingMTX(x_4d, D_4d):\n",
    "    Dp = D_4d[:, :, :2]  # NxNx2\n",
    "    v = x_4d[:, 2:].unsqueeze(1).repeat(1, x_4d.shape[0], 1)  # => NxNx2\n",
    "    DOT_Dp_v = Dp[:, :, 0] * v[:, :, 0] + Dp[:, :, 1] * v[:, :, 1]\n",
    "    COS_THETA = torch.div(DOT_Dp_v, torch.norm(Dp, dim=2) * torch.norm(v, dim=2) + 1E-6)\n",
    "    return COS_THETA\n",
    "\n",
    "\n",
    "def SocialFeatures(x, sub_batches):\n",
    "    N = x.shape[0]  # x is NxTx4 tensor\n",
    "\n",
    "    x_ver_repeat = x[:, -1].unsqueeze(0).repeat(N, 1, 1)\n",
    "    x_hor_repeat = x[:, -1].unsqueeze(1).repeat(1, N, 1)\n",
    "    Dx_mat = x_hor_repeat - x_ver_repeat\n",
    "\n",
    "    l2_dist_MTX = Dx_mat[:, :, :2].norm(dim=2)\n",
    "    bearings_MTX = BearingMTX(x[:, -1], Dx_mat)\n",
    "    dcas_MTX = DCA_MTX(x[:, -1], Dx_mat)\n",
    "    sFeatures_MTX = torch.stack([l2_dist_MTX, bearings_MTX, dcas_MTX], dim=2)\n",
    "\n",
    "    return sFeatures_MTX   # directly return the Social Features Matrix\n",
    "\n",
    "\n",
    "# LSTM path encoding module\n",
    "class EncoderLstm(nn.Module):\n",
    "    def __init__(self, hidden_size, n_layers=2):\n",
    "        # Dimension of the hidden state (h)\n",
    "        self.hidden_size = hidden_size\n",
    "        super(EncoderLstm, self).__init__()\n",
    "        # Linear embedding 4xh\n",
    "        self.embed = nn.Linear(4, self.hidden_size)\n",
    "        # The LSTM cell.\n",
    "        # Input dimension (observations mapped through embedding) is the same as the output\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.lstm_h = []\n",
    "        # init_weights(self)\n",
    "\n",
    "    def init_lstm(self, h, c):\n",
    "        # Initialization of the LSTM: hidden state and cell state\n",
    "        self.lstm_h = (h, c)\n",
    "\n",
    "    def forward(self, obsv):\n",
    "        # Batch size\n",
    "        bs = obsv.shape[0]\n",
    "        # Linear embedding\n",
    "        obsv = self.embed(obsv)\n",
    "        # Reshape and applies LSTM over a whole sequence or over one single step\n",
    "        y, self.lstm_h = self.lstm(obsv.view(bs, -1, self.hidden_size), self.lstm_h)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_next, hidden_dim, n_latent_code):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm_dim = hidden_dim\n",
    "        self.n_next = n_next\n",
    "        # LSTM Encoder for the observed part\n",
    "        self.obsv_encoder_lstm = nn.LSTM(4, hidden_dim, batch_first=True)\n",
    "        # FC sub-network: input is hidden_dim, output is hidden_dim//2. This ouput will be part of\n",
    "        # the input of the classifier.\n",
    "        self.obsv_encoder_fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), nn.LeakyReLU(0.2),\n",
    "                                             nn.Linear(hidden_dim // 2, hidden_dim // 2))\n",
    "        # FC Encoder for the predicted part: input is n_next*4 (whole predicted trajectory), output is\n",
    "        # hidden_dim//2. This ouput will also be part of the input of the classifier.\n",
    "        self.pred_encoder = nn.Sequential(nn.Linear(n_next * 4, hidden_dim // 2), nn.LeakyReLU(0.2),\n",
    "                                          nn.Linear(hidden_dim // 2, hidden_dim // 2))\n",
    "        # Classifier: input is hidden_dim (concatenated encodings of observed and predicted trajectories), output is 1\n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), nn.LeakyReLU(0.2),\n",
    "                                        nn.Linear(hidden_dim // 2, 1))\n",
    "        # Latent code inference: input is hidden_dim (concatenated encodings of observed and predicted trajectories), output is n_latent_code (distribution of latent codes)\n",
    "        self.latent_decoder = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), nn.LeakyReLU(0.2),\n",
    "                                            nn.Linear(self.lstm_dim // 2, n_latent_code))\n",
    "\n",
    "    def forward(self, obsv, pred):\n",
    "        bs = obsv.size(0)\n",
    "        lstm_h_c = (torch.zeros(1, bs, self.lstm_dim).cuda(),\n",
    "                    torch.zeros(1, bs, self.lstm_dim).cuda())\n",
    "        # Encoding of the observed sequence trhough an LSTM cell\n",
    "        obsv_code, lstm_h_c = self.obsv_encoder_lstm(obsv, lstm_h_c)\n",
    "        # Further encoding through a FC layer\n",
    "        obsv_code = self.obsv_encoder_fc(obsv_code[:, -1])\n",
    "        # Encoding of the predicted/next part of the sequence through a FC layer\n",
    "        pred_code = self.pred_encoder(pred.view(-1, self.n_next * 4))\n",
    "        both_codes = torch.cat([obsv_code, pred_code], dim=1)\n",
    "        # Applies classifier to the concatenation of the encodings of both parts\n",
    "        label = self.classifier(both_codes)\n",
    "        # Inference on the latent code\n",
    "        code_hat = self.latent_decoder(both_codes)\n",
    "        return label, code_hat\n",
    "\n",
    "    def load(self, backup):\n",
    "        for m_from, m_to in zip(backup.modules(), self.modules()):\n",
    "            if isinstance(m_to, nn.Linear):\n",
    "                m_to.weight.data = m_from.weight.data.clone()\n",
    "                if m_to.bias is not None:\n",
    "                    m_to.bias.data = m_from.bias.data.clone()\n",
    "\n",
    "\n",
    "# FC path decoding module\n",
    "class DecoderFC(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DecoderFC, self).__init__()\n",
    "        # Fully connected sub-network. Input is hidden_dim, output is 2.\n",
    "        self.fc1 = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(0.2),\n",
    "                                       # torch.nn.Linear(64, 64), nn.LeakyReLU(0.2),\n",
    "                                       torch.nn.Linear(hidden_dim, hidden_dim // 2), nn.LeakyReLU(0.2),\n",
    "                                       torch.nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "                                       torch.nn.Linear(hidden_dim // 4, 2))\n",
    "\n",
    "    def forward(self, h, s, z):\n",
    "        # For each sample in the batch, concatenate h (hidden state), s (social term) and z (noise)\n",
    "        inp = torch.cat([h, s, z], dim=1)\n",
    "        # Applies the fully connected layer\n",
    "        out = self.fc1(inp)\n",
    "        return out\n",
    "\n",
    "\n",
    "# LSTM path decoding module\n",
    "class DecoderLstm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DecoderLstm, self).__init__()\n",
    "        # Decoding LSTM\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        # Fully connected sub-network. Input is hidden_size, output is 2.\n",
    "        self.fc = nn.Sequential(torch.nn.Linear(hidden_size, 64), nn.Sigmoid(),\n",
    "                                torch.nn.Linear(64, 64), nn.LeakyReLU(0.2),\n",
    "                                torch.nn.Linear(64, 32), nn.LeakyReLU(0.2),\n",
    "                                torch.nn.Linear(32, 2))\n",
    "\n",
    "        # init_weights(self)\n",
    "        self.lstm_h = []\n",
    "\n",
    "    def init_lstm(self, h, c):\n",
    "        # Initialization of the LSTM: hidden state and cell state\n",
    "        self.lstm_h = (h, c)\n",
    "\n",
    "    def forward(self, h, s, z):\n",
    "        # Batch size\n",
    "        bs = z.shape[0]\n",
    "        # For each sample in the batch, concatenate h (hidden state), s (social term) and z (noise)\n",
    "        inp = torch.cat([h, s, z], dim=1)\n",
    "        # Applies a forward step.\n",
    "        out, self.lstm_h = self.lstm(inp.unsqueeze(1), self.lstm_h)\n",
    "        # Applies the fully connected layer to the LSTM output\n",
    "        out = self.fc(out.squeeze())\n",
    "        return outtraj-datasets\n",
    "\n",
    "\n",
    "# LSTM-based path encoder\n",
    "encoder = EncoderLstm(hidden_size, n_lstm_layers).cuda()\n",
    "feature_embedder = EmbedSocialFeatures(num_social_features, social_feature_size).cuda()\n",
    "attention = AttentionPooling(hidden_size, social_feature_size).cuda()\n",
    "\n",
    "# Decoder\n",
    "decoder = DecoderFC(hidden_size + social_feature_size + noise_len).cuda()\n",
    "# decoder = DecoderLstm(social_feature_size + VEL_VEC_LEN + noise_len, traj_code_len).cuda()\n",
    "\n",
    "# The Generator parameters and their optimizer\n",
    "predictor_params = chain(attention.parameters(), feature_embedder.parameters(),\n",
    "                         encoder.parameters(), decoder.parameters())\n",
    "predictor_optimizer = opt.Adam(predictor_params, lr=lr_g, betas=(0.9, 0.999))\n",
    "\n",
    "# The Discriminator parameters and their optimizer\n",
    "D = Discriminator(n_next, hidden_size, n_latent_codes).cuda()\n",
    "D_optimizer = opt.Adam(D.parameters(), lr=lr_d, betas=(0.9, 0.999))\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "print('hidden dim = %d | lr(G) =  %.5f | lr(D) =  %.5f' % (hidden_size, lr_g, lr_d))\n",
    "\n",
    "\n",
    "def predict(obsv_p, noise, n_next, sub_batches=[]):\n",
    "    # Batch size\n",
    "    bs = obsv_p.shape[0]\n",
    "    # Adds the velocity component to the observations.\n",
    "    # This makes of obsv_4d a batch_sizexTx4 tensor\n",
    "    obsv_4d = get_traj_4d(obsv_p, [])\n",
    "    # Initial values for the hidden and cell states (zero)\n",
    "    lstm_h_c = (torch.zeros(n_lstm_layers, bs, encoder.hidden_size).cuda(),\n",
    "                torch.zeros(n_lstm_layers, bs, encoder.hidden_size).cuda())\n",
    "    encoder.init_lstm(lstm_h_c[0], lstm_h_c[1])\n",
    "    # Apply the encoder to the observed sequence\n",
    "    # obsv_4d: batch_sizexTx4 tensor\n",
    "    encoder(obsv_4d)\n",
    "    if len(sub_batches) == 0:\n",
    "        sub_batches = [[0, obsv_p.size(0)]]\n",
    "\n",
    "    if use_social:\n",
    "        features = SocialFeatures(obsv_4d, sub_batches)\n",
    "        emb_features = feature_embedder(features, sub_batches)\n",
    "        weighted_features = attention(emb_features, encoder.lstm_h[0].squeeze(), sub_batches)\n",
    "    else:\n",
    "        weighted_features = torch.zeros_like(encoder.lstm_h[0].squeeze())\n",
    "\n",
    "    pred_4ds = []\n",
    "    last_obsv = obsv_4d[:, -1]\n",
    "    # For all the steps to predict, applies a step of the decoder\n",
    "    for ii in range(n_next):\n",
    "        # Takes the current output of the encoder to feed the decoder\n",
    "        # Gets the ouputs as a displacement/velocity\n",
    "        new_v = decoder(encoder.lstm_h[0].view(bs, -1), weighted_features.view(bs, -1), noise).view(bs, 2)\n",
    "        # Deduces the predicted position\n",
    "        new_p = new_v + last_obsv[:, :2]\n",
    "        # The last prediction done will be new_p,new_v\n",
    "        last_obsv = torch.cat([new_p, new_v], dim=1)\n",
    "        # Keeps all the predictions\n",
    "        pred_4ds.append(last_obsv)\n",
    "        # Applies LSTM encoding to the last prediction\n",
    "        # pred_4ds[-1]: batch_sizex4 tensor\n",
    "        encoder(pred_4ds[-1])\n",
    "\n",
    "    return torch.stack(pred_4ds, 1)\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "\n",
    "\n",
    "# =============== Training Loop ==================\n",
    "def train():\n",
    "    tic = time.clock()\n",
    "    # Evaluation metrics (ADE/FDE)\n",
    "    train_ADE, train_FDE = 0, 0\n",
    "    batch_size_accum = 0;\n",
    "    sub_batches = []\n",
    "    # For all the training batches\n",
    "    for ii, batch_i in enumerate(train_batches):\n",
    "        batch_size_accum += batch_i[1] - batch_i[0]\n",
    "        sub_batches.append(batch_i)\n",
    "\n",
    "        # FIXME: Just keep it for toy dataset\n",
    "        # sub_batches = the_batches\n",
    "        # batch_size_accum = sub_batches[-1][1]\n",
    "        # ii = train_size-1\n",
    "\n",
    "        if ii >= train_size - 1 or \\\n",
    "                batch_size_accum + (the_batches[ii + 1][1] - the_batches[ii + 1][0]) > batch_size:\n",
    "            # Observed partial paths\n",
    "            obsv = dataset_obsv[sub_batches[0][0]:sub_batches[-1][1]]\n",
    "            # Future partial paths\n",
    "            pred = dataset_pred[sub_batches[0][0]:sub_batches[-1][1]]\n",
    "            sub_batches = sub_batches - sub_batches[0][0]\n",
    "            # May have to fill with 0\n",
    "            filling_len = batch_size - int(batch_size_accum)\n",
    "            #obsv = torch.cat((obsv, torch.zeros(filling_len, n_past, 2).cuda()), dim=0)\n",
    "            #pred = torch.cat((pred, torch.zeros(filling_len, n_next, 2).cuda()), dim=0)\n",
    "\n",
    "            bs = batch_size_accum\n",
    "\n",
    "            # Completes the positional vectors with velocities (to have dimension 4)\n",
    "            obsv_4d, pred_4d = get_traj_4d(obsv, pred)\n",
    "            zeros = Variable(torch.zeros(bs, 1) + np.random.uniform(0, 0.1), requires_grad=False).cuda()\n",
    "            ones = Variable(torch.ones(bs, 1) * np.random.uniform(0.9, 1.0), requires_grad=False).cuda()\n",
    "            noise = torch.FloatTensor(torch.rand(bs, noise_len)).cuda()\n",
    "            # noise = torch.IntTensor(torch.randint(bs, noise_len)).cude()\n",
    "            # print(\"noise=\", noise.shape, \"\\t\", noise)\n",
    "\n",
    "            # ============== Train Discriminator ================\n",
    "            for u in range(n_unrolling_steps + 1):\n",
    "                # Zero the gradient buffers of all parameters\n",
    "                D.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    pred_hat_4d = predict(obsv, noise, n_next, sub_batches)\n",
    "\n",
    "                fake_labels, code_hat = D(obsv_4d, pred_hat_4d)  # classify fake samples\n",
    "                # Evaluate the MSE loss: the fake_labels should be close to zero\n",
    "                d_loss_fake = mse_loss(fake_labels, zeros)\n",
    "                d_loss_info = mse_loss(code_hat.squeeze(), noise[:, :n_latent_codes])\n",
    "                # Evaluate the MSE loss: the real should be close to one\n",
    "                real_labels, code_hat = D(obsv_4d, pred_4d)  # classify real samples\n",
    "                d_loss_real = mse_loss(real_labels, ones)\n",
    "\n",
    "                #  FIXME: which loss functinos to use for D?\n",
    "                d_loss = d_loss_fake + d_loss_real\n",
    "                if use_info_loss:\n",
    "                    #\n",
    "                    d_loss += loss_info_w * d_loss_info\n",
    "                d_loss.backward()  # update D\n",
    "                D_optimizer.step()\n",
    "\n",
    "                if u == 0 and n_unrolling_steps > 0:\n",
    "                    backup = copy.deepcopy(D)\n",
    "\n",
    "            # =============== Train Generator ================= #\n",
    "            # Zero the gradient buffers of all the discriminator parameters\n",
    "            D.zero_grad()\n",
    "            # Zero the gradient buffers of all the generator parameters\n",
    "            predictor_optimizer.zero_grad()\n",
    "            # Applies a forward step of prediction\n",
    "            pred_hat_4d = predict(obsv, noise, n_next, sub_batches)\n",
    "\n",
    "            # Classify the generated fake sample\n",
    "            gen_labels, code_hat = D(obsv_4d, pred_hat_4d)\n",
    "            # L2 loss between the predicted paths and the true ones\n",
    "            g_loss_l2 = mse_loss(pred_hat_4d[:, :, :2], pred)\n",
    "            # Adversarial loss (classification labels should be close to one)\n",
    "            g_loss_fooling = mse_loss(gen_labels, ones)\n",
    "            # Information loss\n",
    "            g_loss_info = mse_loss(code_hat.squeeze(), noise[:, :n_latent_codes])\n",
    "\n",
    "            #  FIXME: which loss functions to use for G?\n",
    "            #\n",
    "            g_loss = g_loss_fooling\n",
    "            # If using the info loss\n",
    "            if use_info_loss:\n",
    "                g_loss += loss_info_w * g_loss_info\n",
    "            # If using the L2 loss\n",
    "            if use_l2_loss:\n",
    "                g_loss += loss_l2_w * g_loss_l2\n",
    "            if use_variety_loss:\n",
    "                KV = 20\n",
    "                all_20_losses = []\n",
    "                for k in range(KV):\n",
    "                    pred_hat_4d = predict(obsv, noise, n_next, sub_batches)\n",
    "                    loss_l2_k = mse_loss(pred_hat_4d[k, :, :2], pred[k])\n",
    "                all_20_losses.append(loss_l2_k.unsqueeze(0))\n",
    "                all_20_losses = torch.cat(all_20_losses)\n",
    "                variety_loss, _ = torch.min(all_20_losses, dim=0)\n",
    "                g_loss += loss_l2_w * variety_loss\n",
    "\n",
    "            # wandb.log({\n",
    "            #     \"g_loss\":g_loss,\n",
    "            #     \"info_loss\":g_loss_info,\n",
    "            #     \"d_loss\": d_loss\n",
    "            # })\n",
    "            g_loss.backward()\n",
    "            predictor_optimizer.step()\n",
    "\n",
    "            if n_unrolling_steps > 0:\n",
    "                D.load(backup)\n",
    "                del backup\n",
    "\n",
    "            # calculate error\n",
    "            with torch.no_grad():  # TODO: use the function above\n",
    "                err_all = torch.pow((pred_hat_4d[:, :, :2] - pred) / ss, 2)\n",
    "                err_all = err_all.sum(dim=2).sqrt()\n",
    "                e = err_all.sum().item() / n_next\n",
    "                train_ADE += e\n",
    "                train_FDE += err_all[:, -1].sum().item()\n",
    "\n",
    "            batch_size_accum = 0;\n",
    "            sub_batches = []\n",
    "\n",
    "    train_ADE /= n_train_samples\n",
    "    train_FDE /= n_train_samples\n",
    "    toc = time.clock()\n",
    "    # print(\"codes= \", code_hat)\n",
    "    # wandb.log({\n",
    "    #     \"epoch\":epoch,\n",
    "    #     \"ADE\": train_ADE,\n",
    "    #     \"FDE\": train_FDE,\n",
    "    #     \"time\": toc - tic\n",
    "    # })\n",
    "    print(\" Epc=%4d, Train ADE,FDE = (%.3f, %.3f) | time = %.1f\" \\\n",
    "          % (epoch, train_ADE, train_FDE, toc - tic))\n",
    "\n",
    "\n",
    "def test(n_gen_samples=20, linear=False, write_to_file=None, just_one=False):\n",
    "    # =========== Test error ============\n",
    "    plt.close()\n",
    "    ade_avg_12, fde_avg_12 = 0, 0\n",
    "    ade_min_12, fde_min_12 = 0, 0\n",
    "    for ii, batch_i in enumerate(test_batches):\n",
    "        obsv = dataset_obsv[batch_i[0]:batch_i[1]]\n",
    "        pred = dataset_pred[batch_i[0]:batch_i[1]]\n",
    "        current_t = dataset_t[batch_i[0]]\n",
    "        bs = int(batch_i[1] - batch_i[0])\n",
    "        with torch.no_grad():\n",
    "            all_20_errors = []\n",
    "            all_20_preds = []\n",
    "\n",
    "            linear_preds = predict_cv(obsv, n_next)\n",
    "            if linear and not write_to_file:\n",
    "                all_20_preds.append(linear_preds.unsqueeze(0))\n",
    "                err_all = torch.pow((linear_preds[:, :, :2] - pred) / ss, 2).sum(dim=2, keepdim=True).sqrt()\n",
    "                all_20_errors.append(err_all.unsqueeze(0))\n",
    "            else:\n",
    "                for kk in range(n_gen_samples):\n",
    "                    noise = torch.FloatTensor(torch.rand(bs, noise_len)).cuda()\n",
    "                    pred_hat_4d = predict(obsv, noise, n_next)\n",
    "                    all_20_preds.append(pred_hat_4d.unsqueeze(0))\n",
    "                    err_all = torch.pow((pred_hat_4d[:, :, :2] - pred) / ss, 2).sum(dim=2, keepdim=True).sqrt()\n",
    "                    all_20_errors.append(err_all.unsqueeze(0))\n",
    "\n",
    "            all_20_errors = torch.cat(all_20_errors)\n",
    "            if write_to_file:\n",
    "                # file_name = os.path.join(write_to_file, str(epoch) + '-' + str(current_t) + '.npz')\n",
    "                file_name = os.path.join(write_to_file, str(ii) + '-' + str(current_t) + '.npz')\n",
    "                print('saving to ', file_name)\n",
    "                np_obsvs = scale.denormalize(obsv[:, :, :2].data.cpu().numpy())\n",
    "                np_preds_our = scale.denormalize(torch.cat(all_20_preds)[:, :, :, :2].data.cpu().numpy())\n",
    "                np_preds_gtt = scale.denormalize(pred[:, :, :2].data.cpu().numpy())\n",
    "                np_preds_lnr = scale.denormalize(linear_preds[:, :, :2].data.cpu().numpy())\n",
    "                np.savez(file_name, timestamp=current_t,\n",
    "                         obsvs=np_obsvs, preds_our=np_preds_our, preds_gtt=np_preds_gtt, preds_lnr=np_preds_lnr)\n",
    "\n",
    "            # =============== Prediction Errors ================\n",
    "            fde_min_12_i, _ = all_20_errors[:, :, -1].min(0, keepdim=True)\n",
    "            ade_min_12_i, _ = all_20_errors.mean(2).min(0, keepdim=True)\n",
    "            fde_min_12 += fde_min_12_i.sum().item()\n",
    "            ade_min_12 += ade_min_12_i.sum().item()\n",
    "            fde_avg_12 += all_20_errors[:, :, -1].mean(0, keepdim=True).sum().item()\n",
    "            ade_avg_12 += all_20_errors.mean(2).mean(0, keepdim=True).sum().item()\n",
    "            # ==================================================\n",
    "        if just_one: break\n",
    "\n",
    "    ade_avg_12 /= n_test_samples\n",
    "    fde_avg_12 /= n_test_samples\n",
    "    ade_min_12 /= n_test_samples\n",
    "    fde_min_12 /= n_test_samples\n",
    "    print('Avg ADE,FDE (12)= (%.3f, %.3f) | Min(20) ADE,FDE (12)= (%.3f, %.3f)' \\\n",
    "          % (ade_avg_12, fde_avg_12, ade_min_12, fde_min_12))\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# ===================== M A I N =========================\n",
    "# =======================================================\n",
    "if os.path.isfile(model_file):\n",
    "    print('Loading model from ' + model_file)\n",
    "    checkpoint = torch.load(model_file)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    attention.load_state_dict(checkpoint['attentioner_dict'])\n",
    "    feature_embedder.load_state_dict(checkpoint['feature_embedder_dict'])\n",
    "    encoder.load_state_dict(checkpoint['encoder_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_dict'])\n",
    "    predictor_optimizer.load_state_dict(checkpoint['pred_optimizer'])\n",
    "\n",
    "    D.load_state_dict(checkpoint['D_dict'])\n",
    "    D_optimizer.load_state_dict(checkpoint['D_optimizer'])\n",
    "else:\n",
    "    min_train_ADE = 10000\n",
    "    start_epoch = 1\n",
    "\n",
    "# FIXME: comment here to train\n",
    "# wr_dir = '/content/drive/Othercomputers/My MacBook Pro/projs/socialways/preds-iccv/' + dataset_name + '/' + model_name + '/' + str(0000)\n",
    "wr_dir = 'preds-iccv/' + dataset_name + '/' + model_name + '/' + str(0000)\n",
    "os.makedirs(wr_dir, exist_ok=True)\n",
    "test(n_gen_samples=128, write_to_file=wr_dir)\n",
    "exit(1)\n",
    "\n",
    "# ===================== TRAIN =========================\n",
    "# for epoch in trange(start_epoch, n_epochs + 1):  # FIXME : set the number of epochs\n",
    "#     # Main training function\n",
    "#     train()\n",
    "#\n",
    "#     # ============== Save model on disk ===============\n",
    "#     if epoch % 50 == 0:  # FIXME : set the interval for running tests\n",
    "#         print('Saving model to file ...', model_file)\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'attentioner_dict': attention.state_dict(),\n",
    "#             'feature_embedder_dict': feature_embedder.state_dict(),\n",
    "#             'encoder_dict': encoder.state_dict(),\n",
    "#             'decoder_dict': decoder.state_dict(),\n",
    "#             'pred_optimizer': predictor_optimizer.state_dict(),\n",
    "#\n",
    "#             'D_dict': D.state_dict(),\n",
    "#             'D_optimizer': D_optimizer.state_dict()\n",
    "#         }, model_file)\n",
    "#\n",
    "#     if epoch % 500 == 0:\n",
    "#         wr_dir = '/content/drive/Othercomputers/My MacBook Pro/projs/socialways/medium/' + dataset_name + '/' + model_name + '/' + str(epoch)\n",
    "#         os.makedirs(wr_dir, exist_ok=True)\n",
    "#         test(128, write_to_file=wr_dir, just_one=False)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
